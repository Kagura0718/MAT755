---
title: "final"
output: html_document
date: '2022-11-29'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
ObesityDataSet <- read_csv("ObData.csv")
names(ObesityDataSet)
Data <- ObesityDataSet[, -c(3, 4)]
names(Data)
```
```{r}
barplot(table(Data$NObeyesdad))

```
```{r}
#Gender and NO
table.GenNo <- table(ObesityDataSet$Gender, ObesityDataSet$NObeyesdad)
barplot(table.GenNo,beside=T, width=2, legend.text = c("female", "male"),  names.arg =c("Insufficient Weight", "Normal Weight","Overweight Level I", "Overweight Level II", "Obesity Type I", "Obesity Type II", "Obesity Type III"), main = "Barplot of Gender versus Obesity Levels", cex.names = 0.5, xlab = "Obesity Levels", args.legend=list(x="topleft"))

#The Obesity Type II and III are highly correlated with gender

```
```{r}
table.famNo <- table(ObesityDataSet$family_history_with_overweight, ObesityDataSet$NObeyesdad)
barplot(table.famNo,beside=T, width=2, legend.text = c("No", "Yes"),  names.arg =c("Insufficient Weight", "Normal Weight","Overweight Level I", "Overweight Level II", "Obesity Type I", "Obesity Type II", "Obesity Type III"), main = "Barplot of Family History with Overweight versus Obesity Levels", cex.names = 0.5, xlab = "Obesity Levels", args.legend=list(x="topleft"))
```
```{r}
table.CAECNo <- table(ObesityDataSet$CAEC, ObesityDataSet$NObeyesdad)
barplot(table.CAECNo,beside=T, width=2, legend.text = c("No", "Some", "Frequently", "Always"),  names.arg =c("Insufficient Weight", "Normal Weight","Overweight Level I", "Overweight Level II", "Obesity Type I", "Obesity Type II", "Obesity Type III"), main = "Barplot of Consumption of food between meals with Overweight versus Obesity Levels", cex.names = 0.5, xlab = "Obesity Levels", args.legend=list(x="topleft"))
```

```{r}
ObesityDataSet$FCVC = max(ObesityDataSet$FCVC) - ObesityDataSet$FCVC
ObesityDataSet$CH2O = max(ObesityDataSet$CH2O) - ObesityDataSet$CH2O
ObesityDataSet$SCC = max(ObesityDataSet$SCC) - ObesityDataSet$SCC
ObesityDataSet$FAF = max(ObesityDataSet$FAF) - ObesityDataSet$FAF

#factor analysis
fac= factanal(x= Data[,-which(names(Data) %in% "NObeyesdad")], factors=2, scores="regression", rotation = "varimax", method ="mle")
fac


autoplot(fac, data= ObesityDataSet, colour="NObeyesdad",loadings = TRUE, loadings.label = TRUE, loadings.label.size  = 3)
```

```{r}
#PCA
pr = prcomp(Data[,-which(names(Data) %in% "NObeyesdad")], scale. = TRUE)
screeplot(pr, type="lines")
summary(pr,loadings=TRUE)

par(mfrow = c(1, 2))
plot(1:(length(pr$sdev)), (pr$sdev)^2, type='b',
main="Scree Plot", xlab="Number of Components", ylab="Eigenvalue Size")

par(mfrow = c(2, 2))
plot(pr$x[,1], ObesityDataSet$NObeyesdad, pch = 19, xlab = "First PC", ylab = "Obesity level")
plot(pr$x[,2], ObesityDataSet$NObeyesdad, pch = 19, xlab = "Second PC", ylab = "Obesity level")
plot(pr$x[,3], ObesityDataSet$NObeyesdad, pch = 19, xlab = "Third PC", ylab = "Obesity level")
plot(pr$x[,4], ObesityDataSet$NObeyesdad, pch = 19, xlab = "Fourth PC", ylab = "Obesity level")

cor(pr$x, ObesityDataSet$NObeyesdad)

install.packages("ggfortify")
library(ggfortify)
autoplot(pr, data=ObesityDataSet, colour="NObeyesdad", loadings = TRUE, loadings.label = TRUE, loadings.label.size = 3)
```



K-means

```{r}
std= apply(Data[,-which(names(Data) %in% "NObeyesdad")], 2, sd)
Ob.std = sweep(Data[,-which(names(Data) %in% "NObeyesdad")],2,std, FUN="/")
dist = dist(Ob.std)


km2= kmeans(Ob.std, centers=2, iter.max=100, nstart = 25)
table(km2$cluster, ObesityDataSet$NObeyesdad)

km4= kmeans(Ob.std, centers=4, iter.max=100, nstart = 25)
table(km4$cluster, ObesityDataSet$NObeyesdad)


km7= kmeans(Ob.std, centers=7, iter.max=100, nstart = 25)
table(km7$cluster, ObesityDataSet$NObeyesdad)


```

```{r}
my.data.matrix = Ob.std
my.k.choices = 2:10
n = length(my.data.matrix[,1])
wss1 = (n-1)*sum(apply(my.data.matrix,2,var))
wss = numeric(0)

for(i in my.k.choices) {
W = sum(kmeans(my.data.matrix,i)$withinss)
wss = c(wss,W)
}
wss = c(wss1,wss)
plot(c(1,my.k.choices),wss,type='l',xlab='Number of clusters',
ylab='Within-groups sum-of-squares', lwd=2)
```
```{r}
library(psych)
multi.hist(Data[, c(5,6,9,11,12 )],nrow = 2, ncol=2,density=TRUE,freq=FALSE,bcol="lightblue",
      dcol= c("red","blue"),dlty=c("solid", "dotted"),
      main=colnames(data)) 

```

```{r}
ward =hclust(gower.dist, method='ward.D')
plot(ward, labels=F)
cluster<-cutree(ward, k=7)
table(cluster, ObesityDataSet$NObeyesdad)

```

```{r}
#----- Dissimilarity Matrix -----#
#Gower.dist
library(cluster) 
# to perform different types of hierarchical clustering
# package functions used: daisy(), diana(), clusplot()
scaleddata = Data[ , -which(names(Data) %in% "NObeyesdad")]
gower.dist <- daisy(Data[ , -which(names(Data) %in% "NObeyesdad")], metric = c("gower"))

hc<-hclust(gower.dist, method = "complete")
plot(hc, labels=FALSE)
rect.hclust(hc, k=7, border="red")
# choose k, number of clusters 
cluster<-cutree(hc, k=7)
# add cluster to original data 
Data<-cbind(Data,as.factor(cluster))

# class(gower.dist) 
## dissimilarity , dist

#------------ AGGLOMERATIVE CLUSTERING ------------#
# I am looking for the most balanced approach
# Complete linkages is the approach that best fits this demand - I will leave only this one here, don't want to get it cluttered
# complete
aggl.clust.c <- hclust(gower.dist, method = "complete")
plot(aggl.clust.c,
     main = "Agglomerative, complete linkages")

clusterCut = cutree(aggl.clust.c, k = 7)
table(clusterCut, ObesityDataSet$NObeyesdad)



####

silhouette <- c()
silhouette = c(silhouette, NA)
for(i in 2:10){
  pam_clusters = pam(as.matrix(gower.dist),
                 diss = TRUE,
                 k = i)
  silhouette = c(silhouette ,pam_clusters$silinfo$avg.width)
}
plot(1:10, silhouette,
     xlab = "Clusters",
     ylab = "Silhouette Width")
lines(1:10, silhouette)
#the value is maximized at 7. Hence, we can conclude that clustering the data into 5 clusters gives us the best segmentation possible.

#construct a PAM model with 7 clusters, and try to interpret the behavior of these clusters with the help of the medoids.

pam_german = pam(gower.dist, diss = TRUE, k = 7)
Data[pam_german$medoids, ]
table(pam_german$clustering, Data$NObeyesdad)

#The figure above shows the medoids table, where each row represents a cluster. Using this table, we can infer that customers belonging to Cluster 1 have the following characteristics: the duration is 15 months, the credit amount is 1829$, the installment rate is 4, they have been living in the present residence since 4 months, their average age is 46 years, they have 2 loans pending, they have 1 dependent, they have no checking account (A14), their credit history is critical (A34), they are male singles (A93), they have a car as their property (A123), live in their own housing (A152), and have been employed for more or equal to 7 years (A75). Please note that not all customers will be exactly like this; and that the medoids are only a representation of the median values.

fviz_cluster(pam_german, 
             ellipse.type ="euclid",
             repel =TRUE,
             ggtheme =theme_minimal())

```

```{r}
library(cluster)
library(factoextra)
#silhouette analysis
#select the optimal number of clusters
scaleddata = scale(Data[, -which(names(Data) %in% "NObeyesdad")])
fviz_nbclust(scaleddata, pam, method ="silhouette")+theme_minimal()


#the value is maximized at 7. Hence, we can conclude that clustering the data into 5 clusters gives us the best segmentation possible.

#construct a PAM model with 7 clusters, and try to interpret the behavior of these clusters with the help of the medoids.
pam_result = pam(scaleddata, k = 7)
ObesityDataSet$cluster = pam_result$cluster


table(pam_result$clustering, Data$NObeyesdad)

fviz_cluster(pam_result, 
             ellipse.type ="euclid",
             repel =TRUE,
             ggtheme =theme_minimal())



```
LDA

```{r}
library(MASS)
table(ObesityDataSet$NObeyesdad)
g = 7
p =ncol(ObesityDataSet) - 1 
# pooled covariances matrix
Sp = matrix(0, p, p)
nx = rep(0, g)

#Data partition
set.seed(123)
ind <- sample(2, nrow(ObesityDataSet),
              replace = TRUE,
              prob = c(0.6, 0.4))
training <- ObesityDataSet[ind==1,]
testing <- ObesityDataSet[ind==2,]

for(k in 1:g){
  x = ObesityDataSet[ObesityDataSet$NObeyesdad==k,1:p+1]
  nx[k] = nrow(x)
  Sp = Sp + cov(x) * (nx[k] - 1)
}
Sp = Sp / (sum(nx) - g)
round(Sp,3)

# fit lda model
ldamod = lda(NObeyesdad ~ ., data=training)
ldamod
#0.122 belongs to gp1...
#Percentage separations achieved by the first discriminant function is 92.98% and second is 4.66%


# create the (centered) discriminant scores
mu.k = ldamod$means
mu = colMeans(mu.k)
dscores = scale(training[,-17], center=mu, scale=F) %*% ldamod$scaling
sum((dscores - predict(ldamod)$x)^2)

# plot the scores and coefficients
spid = ObesityDataSet$NObeyesdad
par(mfrow=c(1,2))
plot(dscores, xlab="LD1", ylab="LD2", pch=spid, col=spid,
     main="Discriminant Scores", xlim=c(-10, 10), ylim=c(-3, 3))
abline(h=0, lty=3)
abline(v=0, lty=3)
legend("bottomright",toString(c(1:g)),pch=1:3,col=1:3,bty="n")
plot(ldamod$scaling, xlab="LD1", ylab="LD2", type="n",
     main="Discriminant Coefficients", xlim=c(-4, 3), ylim=c(-1, 3))
text(ldamod$scaling, labels=rownames(ldamod$scaling))
abline(h=0, lty=3)
abline(v=0, lty=3)

# visualize the LDA paritions
type = factor(training$NObeyesdad)
library(klaR)
partimat(x=dscores[,2:1], grouping=type, method="lda")

# visualize the LDA paritions (for all pairs)
partimat(x=training[,-17], grouping=type, method="lda")

#Confusion matrix
#For the training dataset
p1 <- predict(ldamod, training)$class
tab <- table(Predicted = p1, Actual = training$NObeyesdad)
tab
sum(diag(tab))/sum(tab)

#
p2 <- predict(ldamod, testing)$class
tab1 <- table(Predicted = p2, Actual = testing$NObeyesdad)
tab1
sum(diag(tab1))/sum(tab1)
```

```{r}
# split into training and testing (100 splits)
X1 = subset(ObesityDataSet, ObesityDataSet$NObeyesdad==1)
X2 = subset(ObesityDataSet, ObesityDataSet$NObeyesdad==2)
X3 = subset(ObesityDataSet, ObesityDataSet$NObeyesdad==3)
X4 = subset(ObesityDataSet, ObesityDataSet$NObeyesdad==4)
X5 = subset(ObesityDataSet, ObesityDataSet$NObeyesdad==5)
X6 = subset(ObesityDataSet, ObesityDataSet$NObeyesdad==6)
X7 = subset(ObesityDataSet, ObesityDataSet$NObeyesdad==7)

nrep = 100
aer = rep(0, nrep)
set.seed(1)
for(k in 1:nrep){
  #cat("rep:",k,"\n")
  id1 = sample.int(n=272, size=50)
  id2 = sample.int(n=287, size=52)
  id3 = sample.int(n=290, size=61)
  id4 = sample.int(n=290, size=60)
  id5 = sample.int(n=351, size=27)
  id6 = sample.int(n=297, size=34)
  id7 = sample.int(n=324, size=73)
  Xtrain = rbind(X1[id1,], X2[id2,], X3[id3,], X4[id4,], X5[id5,], X6[id6,], X7[id7,])
  Xtest = rbind(X1[-id1,], X2[-id2,], X3[-id3,], X4[-id4,], X5[-id5,], X6[-id6,], X7[-id7,])
  ldatrain = lda(NObeyesdad ~ ., data=training, prior=rep(1/7, 7))
  confusionTest = table(testing$NObeyesdad, predict(ldatrain, newdata=testing)$class)
  confusionTest
  n = sum(confusionTest)
  aer[k] = (n - sum(diag(confusionTest))) / n
}
mean(aer)
```

```{r}
ldaid = as.integer(predict(ldamod)$class)
pcamod = princomp(ObesityDataSet[,-17])
par(mfrow=c(1,2))
plot(pcamod$scores[,1:2], xlab="PC1", ylab="PC2", pch=ldaid, col=ldaid,main="LDA Results") 
legend("bottomleft",lev,pch=1:3,col=1:3,bty="n")
abline(h=0,lty=3)
abline(v=0,lty=3)

```

qda
```{r}
# Check equal variance assumption
library(biotools)
boxM(training[,-17],factor(training$NObeyesdad))
# fit qda model
qdamod = qda(NObeyesdad ~ ., data=training,prior=rep(1/7, 7))

names(qdamod)

# check the QDA coefficients/scalings
dim(qdamod$scaling)
dnames = dimnames(qdamod$scaling)
dnames

# visualize the QDA paritions
#partimat(Type ~ ., data=wine, method="qda") # all pairs
partimat(Type ~ ., data=wine[,2:5], method="qda")
partimat(Type ~ ., data=wine[,6:9], method="qda")
partimat(Type ~ ., data=wine[,10:14], method="qda")


```

KNN
```{r}
#install.packages("e1071")
#install.packages("caTools")
#install.packages("class")
library(e1071)
library(caTools)
library(class)

training[,-17] = scale(training[,-17])
testing[,-17] = scale(testing[,-17])

classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = training$NObeyesdad,
                      k = 1)
classifier_knn

# Confusiin Matrix
cm <- table(testing$NObeyesdad, classifier_knn)
cm

# Model Evaluation - Choosing K
# Calculate out of Sample error
misClassError <- mean(classifier_knn != testing$NObeyesdad)
print(paste('Accuracy =', 1-misClassError))
  
for (i in 1: 20){
  classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = training$NObeyesdad,
                      k = i)
  misClassError[i] <- mean(classifier_knn != testing$NObeyesdad)
}

plot(c(1:20), misClassError)

  
# K = 7
classifier_knn <- knn(train = train_scale,
                      test = test_scale,
                      cl = training$NObeyesdad,
                      k = 7)
misClassError <- mean(classifier_knn != testing$NObeyesdad)
print(paste('Accuracy =', 1-misClassError))
```
SVM
```{r}
classifier = svm(formula = NObeyesdad ~ .,
                 data = training,
                 type = 'C-classification',
                 kernel = 'linear')

y_pred = predict(classifier, newdata = testing[, -17])

train_pred = predict(classifier, newdata = training[, -17])
cm1 = table(training$NObeyesdad, train_pred)
sum(diag(cm1))/sum(cm1)

# Making the Confusion Matrix
cm = table(testing$NObeyesdad, y_pred)
sum(diag(cm))/sum(cm)

#visualize

```



